diff --git a/LICENSE b/LICENSE
index bc492ae..5a90478 100644
--- a/LICENSE
+++ b/LICENSE
@@ -1,21 +1,201 @@
-MIT License
-
-Copyright (c) 2020 ying09
-
-Permission is hereby granted, free of charge, to any person obtaining a copy
-of this software and associated documentation files (the "Software"), to deal
-in the Software without restriction, including without limitation the rights
-to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
-copies of the Software, and to permit persons to whom the Software is
-furnished to do so, subject to the following conditions:
-
-The above copyright notice and this permission notice shall be included in all
-copies or substantial portions of the Software.
-
-THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
-IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
-FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
-AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
-LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
-OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
-SOFTWARE.
+Apache License
+Version 2.0, January 2004
+http://www.apache.org/licenses/
+
+TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
+
+1. Definitions.
+
+"License" shall mean the terms and conditions for use, reproduction,
+and distribution as defined by Sections 1 through 9 of this document.
+
+"Licensor" shall mean the copyright owner or entity authorized by
+the copyright owner that is granting the License.
+
+"Legal Entity" shall mean the union of the acting entity and all
+other entities that control, are controlled by, or are under common
+control with that entity. For the purposes of this definition,
+"control" means (i) the power, direct or indirect, to cause the
+direction or management of such entity, whether by contract or
+otherwise, or (ii) ownership of fifty percent (50%) or more of the
+outstanding shares, or (iii) beneficial ownership of such entity.
+
+"You" (or "Your") shall mean an individual or Legal Entity
+exercising permissions granted by this License.
+
+"Source" form shall mean the preferred form for making modifications,
+including but not limited to software source code, documentation
+source, and configuration files.
+
+"Object" form shall mean any form resulting from mechanical
+transformation or translation of a Source form, including but
+not limited to compiled object code, generated documentation,
+and conversions to other media types.
+
+"Work" shall mean the work of authorship, whether in Source or
+Object form, made available under the License, as indicated by a
+copyright notice that is included in or attached to the work
+(an example is provided in the Appendix below).
+
+"Derivative Works" shall mean any work, whether in Source or Object
+form, that is based on (or derived from) the Work and for which the
+editorial revisions, annotations, elaborations, or other modifications
+represent, as a whole, an original work of authorship. For the purposes
+of this License, Derivative Works shall not include works that remain
+separable from, or merely link (or bind by name) to the interfaces of,
+the Work and Derivative Works thereof.
+
+"Contribution" shall mean any work of authorship, including
+the original version of the Work and any modifications or additions
+to that Work or Derivative Works thereof, that is intentionally
+submitted to Licensor for inclusion in the Work by the copyright owner
+or by an individual or Legal Entity authorized to submit on behalf of
+the copyright owner. For the purposes of this definition, "submitted"
+means any form of electronic, verbal, or written communication sent
+to the Licensor or its representatives, including but not limited to
+communication on electronic mailing lists, source code control systems,
+and issue tracking systems that are managed by, or on behalf of, the
+Licensor for the purpose of discussing and improving the Work, but
+excluding communication that is conspicuously marked or otherwise
+designated in writing by the copyright owner as "Not a Contribution."
+
+"Contributor" shall mean Licensor and any individual or Legal Entity
+on behalf of whom a Contribution has been received by Licensor and
+subsequently incorporated within the Work.
+
+2. Grant of Copyright License. Subject to the terms and conditions of
+this License, each Contributor hereby grants to You a perpetual,
+worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+copyright license to reproduce, prepare Derivative Works of,
+publicly display, publicly perform, sublicense, and distribute the
+Work and such Derivative Works in Source or Object form.
+
+3. Grant of Patent License. Subject to the terms and conditions of
+this License, each Contributor hereby grants to You a perpetual,
+worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+(except as stated in this section) patent license to make, have made,
+use, offer to sell, sell, import, and otherwise transfer the Work,
+where such license applies only to those patent claims licensable
+by such Contributor that are necessarily infringed by their
+Contribution(s) alone or by combination of their Contribution(s)
+with the Work to which such Contribution(s) was submitted. If You
+institute patent litigation against any entity (including a
+cross-claim or counterclaim in a lawsuit) alleging that the Work
+or a Contribution incorporated within the Work constitutes direct
+or contributory patent infringement, then any patent licenses
+granted to You under this License for that Work shall terminate
+as of the date such litigation is filed.
+
+4. Redistribution. You may reproduce and distribute copies of the
+Work or Derivative Works thereof in any medium, with or without
+modifications, and in Source or Object form, provided that You
+meet the following conditions:
+
+(a) You must give any other recipients of the Work or
+Derivative Works a copy of this License; and
+
+(b) You must cause any modified files to carry prominent notices
+stating that You changed the files; and
+
+(c) You must retain, in the Source form of any Derivative Works
+that You distribute, all copyright, patent, trademark, and
+attribution notices from the Source form of the Work,
+excluding those notices that do not pertain to any part of
+the Derivative Works; and
+
+(d) If the Work includes a "NOTICE" text file as part of its
+distribution, then any Derivative Works that You distribute must
+include a readable copy of the attribution notices contained
+within such NOTICE file, excluding those notices that do not
+pertain to any part of the Derivative Works, in at least one
+of the following places: within a NOTICE text file distributed
+as part of the Derivative Works; within the Source form or
+documentation, if provided along with the Derivative Works; or,
+within a display generated by the Derivative Works, if and
+wherever such third-party notices normally appear. The contents
+of the NOTICE file are for informational purposes only and
+do not modify the License. You may add Your own attribution
+notices within Derivative Works that You distribute, alongside
+or as an addendum to the NOTICE text from the Work, provided
+that such additional attribution notices cannot be construed
+as modifying the License.
+
+You may add Your own copyright statement to Your modifications and
+may provide additional or different license terms and conditions
+for use, reproduction, or distribution of Your modifications, or
+for any such Derivative Works as a whole, provided Your use,
+reproduction, and distribution of the Work otherwise complies with
+the conditions stated in this License.
+
+5. Submission of Contributions. Unless You explicitly state otherwise,
+any Contribution intentionally submitted for inclusion in the Work
+by You to the Licensor shall be under the terms and conditions of
+this License, without any additional terms or conditions.
+Notwithstanding the above, nothing herein shall supersede or modify
+the terms of any separate license agreement you may have executed
+with Licensor regarding such Contributions.
+
+6. Trademarks. This License does not grant permission to use the trade
+names, trademarks, service marks, or product names of the Licensor,
+except as required for reasonable and customary use in describing the
+origin of the Work and reproducing the content of the NOTICE file.
+
+7. Disclaimer of Warranty. Unless required by applicable law or
+agreed to in writing, Licensor provides the Work (and each
+Contributor provides its Contributions) on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+implied, including, without limitation, any warranties or conditions
+of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
+PARTICULAR PURPOSE. You are solely responsible for determining the
+appropriateness of using or redistributing the Work and assume any
+risks associated with Your exercise of permissions under this License.
+
+8. Limitation of Liability. In no event and under no legal theory,
+whether in tort (including negligence), contract, or otherwise,
+unless required by applicable law (such as deliberate and grossly
+negligent acts) or agreed to in writing, shall any Contributor be
+liable to You for damages, including any direct, indirect, special,
+incidental, or consequential damages of any character arising as a
+result of this License or out of the use or inability to use the
+Work (including but not limited to damages for loss of goodwill,
+work stoppage, computer failure or malfunction, or any and all
+other commercial damages or losses), even if such Contributor
+has been advised of the possibility of such damages.
+
+9. Accepting Warranty or Additional Liability. While redistributing
+the Work or Derivative Works thereof, You may choose to offer,
+and charge a fee for, acceptance of support, warranty, indemnity,
+or other liability obligations and/or rights consistent with this
+License. However, in accepting such obligations, You may act only
+on Your own behalf and on Your sole responsibility, not on behalf
+of any other Contributor, and only if You agree to indemnify,
+defend, and hold each Contributor harmless for any liability
+incurred by, or claims asserted against, such Contributor by reason
+of your accepting any such warranty or additional liability.
+
+END OF TERMS AND CONDITIONS
+
+APPENDIX: How to apply the Apache License to your work.
+
+To apply the Apache License to your work, attach the following
+boilerplate notice, with the fields enclosed by brackets "[]"
+replaced with your own identifying information. (Don't include
+the brackets!)  The text should be enclosed in the appropriate
+comment syntax for the file format. We also recommend that a
+file or class name and description of purpose be included on the
+same "printed page" as the copyright notice for easier
+identification within third-party archives.
+
+Copyright 2019, Facebook, Inc
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
diff --git a/README.md b/README.md
index d15bc21..404b49b 100644
--- a/README.md
+++ b/README.md
@@ -1,78 +1,56 @@
-# [IJCAI 2020]  TextFuseNet: Scene Text Detection with Richer Fused Features 
-This software implements TextFuseNet: Scene Text Detection with Richer Fused Features in PyTorch. For more details, please refer to our paper https://www.ijcai.org/Proceedings/2020/72.
-
-## Abstract
-Arbitrary shape text detection in natural scenes is an extremely challenging task. Unlike existing text detection approaches that only perceive texts based on limited feature representations, we propose a novel framework, namely TextFuseNet, to exploit the use of richer features fused for text detection. More specifically, we propose to perceive texts from three levels of feature representations, i.e., character-, word- and global-level, and then introduce a novel text representation fusion technique to help achieve robust arbitrary text detection. The multi-level feature representation can adequately describe texts by dissecting them into individual characters while still maintaining their general semantics. TextFuseNet then collects and merges the texts’ features from different levels using a multi-path fusion architecture which can effectively align and fuse different representations. In practice, our proposed TextFuseNet can learn a more adequate description of arbitrary shapes texts, suppressing false positives and producing more accurate detection results. Our proposed framework can also be trained with weak supervision for those datasets that lack character-level annotations. Experiments on several datasets show that the proposed TextFuseNet achieves state-of-the-art performance. Specifically, we achieve an F-measure of 94.3% on ICDAR2013, 92.1% on ICDAR2015, 87.1% on Total-Text and 86.6% on CTW-1500, respectively.
-
-![image](https://github.com/ying09/TextFuseNet.pytorch/blob/master/TextFuseNet.jpg)
-
-# Installation
-This implementation is based on [Detectron2](https://github.com/facebookresearch/detectron2), the installation can refer to [step-by-step installation.txt](https://github.com/ying09/TextFuseNet/blob/master/step-by-step%20installation.txt). For more details about the environment of conda, please refer to [requirements.txt](https://github.com/ying09/TextFuseNet/blob/master/requirements.txt).
-
-# Docker
-There is also a Dockerfile for testing purposes available. See [docker](docker).
-
-# Run demo
-A demo program can be found in demo. Before running the demo, download our pretrained models from [Baidu Netdisk](https://pan.baidu.com/s/1wSjZPRh3SL1rpNMtZSHodQ) (Extraction code:8op1) or [Google Driver](https://drive.google.com/drive/folders/18Ll-3bAmi4CR2eGTuM-j6fkMrSAaBV4Z?usp=sharing). Set the path of files (include model, testing images, configs, output etc.) in demo/***_detection.py.  Then launch demo by:
-    
-    python demo/icdar2013_detection.py
-
-# Evaluation
-Our detection code will save text contours to a txt file for each image. For calculating F-measure, Recall, and Precision, please refer to the following links:  
-[ICDAR2013](https://rrc.cvc.uab.es/?ch=2)  
-[ICDAR2015](https://rrc.cvc.uab.es/?ch=4)  
-[Total-Text](https://github.com/cs-chan/Total-Text-Dataset)  
-[CTW-1500](https://github.com/Yuliang-Liu/Curve-Text-Detector)  
-[ICDAR2019-ArT](https://rrc.cvc.uab.es/?ch=14)
-
-# Train a new model
-Before training，please register your datasets in detectron2/data/datasets/builtin.py. Set training implementation details in configs/ocr/***.yaml.  To train a model with 4 gpus，please run:
-
-    python tools/train_net.py --num-gpus 4 --config-file configs/ocr/icdar2013_101_FPN.yaml
-
-# Annotation example
-The annotation example can be found in [annotation_example](https://github.com/ying09/TextFuseNet/tree/master/annotation_example).
-For word-level labels and character-level labels, please see corresponding details of weakly supervised learning method in our paper. 
-For semantic segmentation labels, we generate it according to the masks of text instances during training, and for more details, please see corresponding code in [seg_head.py](https://github.com/ying09/TextFuseNet/blob/master/detectron2/modeling/roi_heads/seg_head.py).
-
-# Results
-Example results of TextFuseNet on different datasets.
-
-![image](https://github.com/ying09/TextFuseNet/blob/master/example_results.png)
-
-Evaluation of TextFuseNet on different datasets with ResNet-101/ResNeXt-101 backbone:
-|Datasets|Model|Recall|Precision|F-measure|
-|:------:|:------:|:------:|:------:|:------:|
-|totaltext|Paper (ResNet-101)|85.3|89.0|87.1|
-|totaltext|This implementation (ResNeXt-101)|__85.8__|__89.2__|__87.5__|
-|ctw1500|Paper (ResNet-101)|85.4|87.8|86.6|
-|ctw1500|This implementation (ResNeXt-101)|85.1|__89.7__|__87.4__|
-|icdar2013|Paper (ResNet-101)|92.3|96.5|94.3|
-|icdar2013|This implementation (ResNeXt-101)|92.1|__97.2__|__94.6__|
-|icdar2015|Paper (ResNet-101)|89.7|94.7|92.1|
-|icdar2015|This implementation (ResNeXt-101)|__90.6__|94.0|__92.2__|
-|icdar2019-ArT|This implementation (ResNeXt-101)|72.8|85.4|78.6|
-
-Evaluation of TextFuseNet on different datasets with ResNet-50 backbone:
-|Datasets|Model|Recall|Precision|F-measure|
-|:------:|:------:|:------:|:------:|:------:|
-|totaltext|Paper|83.2|87.5|85.3|
-|ctw1500|Paper|85.0|85.8|85.4|
-|icdar2013|Paper|89.5|95.1|92.2|
-|icdar2015|Paper|88.9|91.3|90.1|
-|icdar2019-ArT|This implementation|69.4|82.6|75.4|
-
-
-# Citation
-    @inproceedings{ijcai2020-72,  
-        title={TextFuseNet: Scene Text Detection with Richer Fused Features},  
-        author={Ye, Jian and Chen, Zhe and Liu, Juhua and Du, Bo},   
-        booktitle={Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, {IJCAI-20}},     
-        publisher={International Joint Conferences on Artificial Intelligence Organization},     
-        pages={516--522},     
-        year={2020}     
-    }
-
-# Acknowledgements
-The authors would like to thank the developers of PyTorch and Detectron2. See [LICENSE](https://github.com/ying09/TextFuseNet/blob/master/LICENSE) for additional details.  
-Please let me know if you encounter any issues.
+<img src=".github/Detectron2-Logo-Horz.svg" width="300" >
+
+Detectron2 is Facebook AI Research's next generation software system
+that implements state-of-the-art object detection algorithms.
+It is a ground-up rewrite of the previous version,
+[Detectron](https://github.com/facebookresearch/Detectron/),
+and it originates from [maskrcnn-benchmark](https://github.com/facebookresearch/maskrcnn-benchmark/).
+
+<div align="center">
+  <img src="https://user-images.githubusercontent.com/1381301/66535560-d3422200-eace-11e9-9123-5535d469db19.png"/>
+</div>
+
+### What's New
+* It is powered by the [PyTorch](https://pytorch.org) deep learning framework.
+* Includes more features such as panoptic segmentation, densepose, Cascade R-CNN, rotated bounding boxes, etc.
+* Can be used as a library to support [different projects](projects/) on top of it.
+  We'll open source more research projects in this way.
+* It [trains much faster](https://detectron2.readthedocs.io/notes/benchmarks.html).
+
+See our [blog post](https://ai.facebook.com/blog/-detectron2-a-pytorch-based-modular-object-detection-library-/)
+to see more demos and learn about detectron2.
+
+## Installation
+
+See [INSTALL.md](INSTALL.md).
+
+## Quick Start
+
+See [GETTING_STARTED.md](GETTING_STARTED.md),
+or the [Colab Notebook](https://colab.research.google.com/drive/16jcaJoc6bCFAQ96jDe2HwtXj7BMD_-m5).
+
+Learn more at our [documentation](https://detectron2.readthedocs.org).
+And see [projects/](projects/) for some projects that are built on top of detectron2.
+
+## Model Zoo and Baselines
+
+We provide a large set of baseline results and trained models available for download in the [Detectron2 Model Zoo](MODEL_ZOO.md).
+
+
+## License
+
+Detectron2 is released under the [Apache 2.0 license](LICENSE).
+
+## Citing Detectron
+
+If you use Detectron2 in your research or wish to refer to the baseline results published in the [Model Zoo](MODEL_ZOO.md), please use the following BibTeX entry.
+
+```BibTeX
+@misc{wu2019detectron2,
+  author =       {Yuxin Wu and Alexander Kirillov and Francisco Massa and
+                  Wan-Yen Lo and Ross Girshick},
+  title =        {Detectron2},
+  howpublished = {\url{https://github.com/facebookresearch/detectron2}},
+  year =         {2019}
+}
+```
diff --git a/demo/predictor.py b/demo/predictor.py
index f2dc4f6..689fa85 100644
--- a/demo/predictor.py
+++ b/demo/predictor.py
@@ -61,9 +61,9 @@ class VisualizationDemo(object):
                 )
             if "instances" in predictions:
                 instances = predictions["instances"].to(self.cpu_device)
-                vis_output,polygons = visualizer.draw_instance_predictions(predictions=instances)
+                vis_output = visualizer.draw_instance_predictions(predictions=instances)
 
-        return predictions, vis_output, polygons
+        return predictions, vis_output
 
     def _frame_from_video(self, video):
         while video.isOpened():
diff --git a/detectron2/config/defaults.py b/detectron2/config/defaults.py
index 8125370..323af6e 100644
--- a/detectron2/config/defaults.py
+++ b/detectron2/config/defaults.py
@@ -23,11 +23,9 @@ _C.MODEL = CN()
 _C.MODEL.LOAD_PROPOSALS = False
 _C.MODEL.MASK_ON = False
 _C.MODEL.KEYPOINT_ON = False
-_C.MODEL.TEXTFUSENET_MUTIL_PATH_FUSE_ON = False
 _C.MODEL.DEVICE = "cuda"
 _C.MODEL.META_ARCHITECTURE = "GeneralizedRCNN"
 
-
 # If the WEIGHT starts with a catalog://, like :R-50, the code will look for
 # the path in ModelCatalog. Else, it will use it as the specified absolute
 # path
@@ -42,8 +40,6 @@ _C.MODEL.PIXEL_MEAN = [103.530, 116.280, 123.675]
 _C.MODEL.PIXEL_STD = [1.0, 1.0, 1.0]
 
 
-
-
 # -----------------------------------------------------------------------------
 # INPUT
 # -----------------------------------------------------------------------------
@@ -234,7 +230,7 @@ _C.MODEL.RPN.NMS_THRESH = 0.7
 _C.MODEL.ROI_HEADS = CN()
 _C.MODEL.ROI_HEADS.NAME = "Res5ROIHeads"
 # Number of foreground classes
-_C.MODEL.ROI_HEADS.NUM_CLASSES = 64
+_C.MODEL.ROI_HEADS.NUM_CLASSES = 80
 # Names of the input feature maps to be used by ROI heads
 # Currently all heads (box, mask, ...) use the same input feature map list
 # e.g., ["p2", "p3", "p4", "p5"] is commonly used for FPN
@@ -281,8 +277,7 @@ _C.MODEL.ROI_BOX_HEAD.BBOX_REG_WEIGHTS = (10.0, 10.0, 5.0, 5.0)
 # The transition point from L1 to L2 loss. Set to 0.0 to make the loss simply L1.
 _C.MODEL.ROI_BOX_HEAD.SMOOTH_L1_BETA = 0.0
 _C.MODEL.ROI_BOX_HEAD.POOLER_RESOLUTION = 14
-#2020-4-29 yejian 0->2
-_C.MODEL.ROI_BOX_HEAD.POOLER_SAMPLING_RATIO = 2
+_C.MODEL.ROI_BOX_HEAD.POOLER_SAMPLING_RATIO = 0
 # Type of pooling operation applied to the incoming feature map for each RoI
 _C.MODEL.ROI_BOX_HEAD.POOLER_TYPE = "ROIAlignV2"
 
@@ -317,8 +312,7 @@ _C.MODEL.ROI_BOX_CASCADE_HEAD.IOUS = (0.5, 0.6, 0.7)
 _C.MODEL.ROI_MASK_HEAD = CN()
 _C.MODEL.ROI_MASK_HEAD.NAME = "MaskRCNNConvUpsampleHead"
 _C.MODEL.ROI_MASK_HEAD.POOLER_RESOLUTION = 14
-#2020-4-29 yejian 0->2
-_C.MODEL.ROI_MASK_HEAD.POOLER_SAMPLING_RATIO = 2
+_C.MODEL.ROI_MASK_HEAD.POOLER_SAMPLING_RATIO = 0
 _C.MODEL.ROI_MASK_HEAD.NUM_CONV = 0  # The number of convs in the mask head
 _C.MODEL.ROI_MASK_HEAD.CONV_DIM = 256
 # Normalization method for the convolution layers.
@@ -365,25 +359,6 @@ _C.MODEL.ROI_KEYPOINT_HEAD.LOSS_WEIGHT = 1.0
 # Type of pooling operation applied to the incoming feature map for each RoI
 _C.MODEL.ROI_KEYPOINT_HEAD.POOLER_TYPE = "ROIAlignV2"
 
-# ---------------------------------------------------------------------------- #
-# TextFuseNet Seg Head
-# ---------------------------------------------------------------------------- #
-_C.MODEL.TEXTFUSENET_SEG_HEAD = CN()
-_C.MODEL.TEXTFUSENET_SEG_HEAD.CHANNELS = 256
-# the num of fpn features used to fuse
-_C.MODEL.TEXTFUSENET_SEG_HEAD.NUM_FPN_FEATURES = 4
-# the num of conv3x3 used to extract fused features
-_C.MODEL.TEXTFUSENET_SEG_HEAD.NUM_CONV3 = 4
-# the num of segmentic classes 
-_C.MODEL.TEXTFUSENET_SEG_HEAD.NUM_CLASSES =2
-
-_C.MODEL.TEXTFUSENET_SEG_HEAD.POOLER_RESOLUTION = 14
-_C.MODEL.TEXTFUSENET_SEG_HEAD.SAMPLING_RATIO = 2
-_C.MODEL.TEXTFUSENET_SEG_HEAD.POOLER_TYPE = "ROIAlignV2"
-# 0->0.25, 1->0.125, 2->0.0625, 3->0.03125
-_C.MODEL.TEXTFUSENET_SEG_HEAD.FPN_FEATURES_FUSED_LEVEL = 2
-_C.MODEL.TEXTFUSENET_SEG_HEAD.POOLER_SCALES = (0.0625,)
-
 # ---------------------------------------------------------------------------- #
 # Semantic Segmentation Head
 # ---------------------------------------------------------------------------- #
diff --git a/detectron2/data/datasets/builtin.py b/detectron2/data/datasets/builtin.py
index 1b13e8e..28ec435 100644
--- a/detectron2/data/datasets/builtin.py
+++ b/detectron2/data/datasets/builtin.py
@@ -44,13 +44,6 @@ _PREDEFINED_SPLITS_COCO["coco"] = {
     "coco_2017_test": ("coco/test2017", "coco/annotations/image_info_test2017.json"),
     "coco_2017_test-dev": ("coco/test2017", "coco/annotations/image_info_test-dev2017.json"),
     "coco_2017_val_100": ("coco/val2017", "coco/annotations/instances_val2017_100.json"),
-
-    # ocr datasets register    
-    "totaltext":("totaltext/train_images", "totaltext/train.json"),
-    "ctw1500":("ctw1500/train_images", "ctw1500/train.json"),
-    "icdar2013":("icdar2013/train_images", "icdar2013/train.json"),
-    "icdar2015":("icdar2015/train_images", "icdar2015/train.json"),
-
 }
 
 _PREDEFINED_SPLITS_COCO["coco_person"] = {
@@ -108,7 +101,7 @@ _PREDEFINED_SPLITS_COCO_PANOPTIC = {
 }
 
 
-def register_all_coco(root="***"):  # put your root path of ocr datasets here
+def register_all_coco(root="datasets"):
     for dataset_name, splits_per_dataset in _PREDEFINED_SPLITS_COCO.items():
         for key, (image_root, json_file) in splits_per_dataset.items():
             # Assume pre-defined datasets live in `./datasets`.
diff --git a/detectron2/data/datasets/builtin_meta.py b/detectron2/data/datasets/builtin_meta.py
index afef5bf..74c7986 100644
--- a/detectron2/data/datasets/builtin_meta.py
+++ b/detectron2/data/datasets/builtin_meta.py
@@ -4,216 +4,142 @@
 
 # All coco categories, together with their nice-looking visualization colors
 # It's from https://github.com/cocodataset/panopticapi/blob/master/panoptic_coco_categories.json
-# COCO_CATEGORIES = [
-#     {"color": [220, 20, 60], "isthing": 1, "id": 1, "name": "person"},
-#     {"color": [119, 11, 32], "isthing": 1, "id": 2, "name": "bicycle"},
-#     {"color": [0, 0, 142], "isthing": 1, "id": 3, "name": "car"},
-#     {"color": [0, 0, 230], "isthing": 1, "id": 4, "name": "motorcycle"},
-#     {"color": [106, 0, 228], "isthing": 1, "id": 5, "name": "airplane"},
-#     {"color": [0, 60, 100], "isthing": 1, "id": 6, "name": "bus"},
-#     {"color": [0, 80, 100], "isthing": 1, "id": 7, "name": "train"},
-#     {"color": [0, 0, 70], "isthing": 1, "id": 8, "name": "truck"},
-#     {"color": [0, 0, 192], "isthing": 1, "id": 9, "name": "boat"},
-#     {"color": [250, 170, 30], "isthing": 1, "id": 10, "name": "traffic light"},
-#     {"color": [100, 170, 30], "isthing": 1, "id": 11, "name": "fire hydrant"},
-#     {"color": [220, 220, 0], "isthing": 1, "id": 13, "name": "stop sign"},
-#     {"color": [175, 116, 175], "isthing": 1, "id": 14, "name": "parking meter"},
-#     {"color": [250, 0, 30], "isthing": 1, "id": 15, "name": "bench"},
-#     {"color": [165, 42, 42], "isthing": 1, "id": 16, "name": "bird"},
-#     {"color": [255, 77, 255], "isthing": 1, "id": 17, "name": "cat"},
-#     {"color": [0, 226, 252], "isthing": 1, "id": 18, "name": "dog"},
-#     {"color": [182, 182, 255], "isthing": 1, "id": 19, "name": "horse"},
-#     {"color": [0, 82, 0], "isthing": 1, "id": 20, "name": "sheep"},
-#     {"color": [120, 166, 157], "isthing": 1, "id": 21, "name": "cow"},
-#     {"color": [110, 76, 0], "isthing": 1, "id": 22, "name": "elephant"},
-#     {"color": [174, 57, 255], "isthing": 1, "id": 23, "name": "bear"},
-#     {"color": [199, 100, 0], "isthing": 1, "id": 24, "name": "zebra"},
-#     {"color": [72, 0, 118], "isthing": 1, "id": 25, "name": "giraffe"},
-#     {"color": [255, 179, 240], "isthing": 1, "id": 27, "name": "backpack"},
-#     {"color": [0, 125, 92], "isthing": 1, "id": 28, "name": "umbrella"},
-#     {"color": [209, 0, 151], "isthing": 1, "id": 31, "name": "handbag"},
-#     {"color": [188, 208, 182], "isthing": 1, "id": 32, "name": "tie"},
-#     {"color": [0, 220, 176], "isthing": 1, "id": 33, "name": "suitcase"},
-#     {"color": [255, 99, 164], "isthing": 1, "id": 34, "name": "frisbee"},
-#     {"color": [92, 0, 73], "isthing": 1, "id": 35, "name": "skis"},
-#     {"color": [133, 129, 255], "isthing": 1, "id": 36, "name": "snowboard"},
-#     {"color": [78, 180, 255], "isthing": 1, "id": 37, "name": "sports ball"},
-#     {"color": [0, 228, 0], "isthing": 1, "id": 38, "name": "kite"},
-#     {"color": [174, 255, 243], "isthing": 1, "id": 39, "name": "baseball bat"},
-#     {"color": [45, 89, 255], "isthing": 1, "id": 40, "name": "baseball glove"},
-#     {"color": [134, 134, 103], "isthing": 1, "id": 41, "name": "skateboard"},
-#     {"color": [145, 148, 174], "isthing": 1, "id": 42, "name": "surfboard"},
-#     {"color": [255, 208, 186], "isthing": 1, "id": 43, "name": "tennis racket"},
-#     {"color": [197, 226, 255], "isthing": 1, "id": 44, "name": "bottle"},
-#     {"color": [171, 134, 1], "isthing": 1, "id": 46, "name": "wine glass"},
-#     {"color": [109, 63, 54], "isthing": 1, "id": 47, "name": "cup"},
-#     {"color": [207, 138, 255], "isthing": 1, "id": 48, "name": "fork"},
-#     {"color": [151, 0, 95], "isthing": 1, "id": 49, "name": "knife"},
-#     {"color": [9, 80, 61], "isthing": 1, "id": 50, "name": "spoon"},
-#     {"color": [84, 105, 51], "isthing": 1, "id": 51, "name": "bowl"},
-#     {"color": [74, 65, 105], "isthing": 1, "id": 52, "name": "banana"},
-#     {"color": [166, 196, 102], "isthing": 1, "id": 53, "name": "apple"},
-#     {"color": [208, 195, 210], "isthing": 1, "id": 54, "name": "sandwich"},
-#     {"color": [255, 109, 65], "isthing": 1, "id": 55, "name": "orange"},
-#     {"color": [0, 143, 149], "isthing": 1, "id": 56, "name": "broccoli"},
-#     {"color": [179, 0, 194], "isthing": 1, "id": 57, "name": "carrot"},
-#     {"color": [209, 99, 106], "isthing": 1, "id": 58, "name": "hot dog"},
-#     {"color": [5, 121, 0], "isthing": 1, "id": 59, "name": "pizza"},
-#     {"color": [227, 255, 205], "isthing": 1, "id": 60, "name": "donut"},
-#     {"color": [147, 186, 208], "isthing": 1, "id": 61, "name": "cake"},
-#     {"color": [153, 69, 1], "isthing": 1, "id": 62, "name": "chair"},
-#     {"color": [3, 95, 161], "isthing": 1, "id": 63, "name": "couch"},
-#     {"color": [163, 255, 0], "isthing": 1, "id": 64, "name": "potted plant"},
-#     {"color": [119, 0, 170], "isthing": 1, "id": 65, "name": "bed"},
-#     {"color": [0, 182, 199], "isthing": 1, "id": 67, "name": "dining table"},
-#     {"color": [0, 165, 120], "isthing": 1, "id": 70, "name": "toilet"},
-#     {"color": [183, 130, 88], "isthing": 1, "id": 72, "name": "tv"},
-#     {"color": [95, 32, 0], "isthing": 1, "id": 73, "name": "laptop"},
-#     {"color": [130, 114, 135], "isthing": 1, "id": 74, "name": "mouse"},
-#     {"color": [110, 129, 133], "isthing": 1, "id": 75, "name": "remote"},
-#     {"color": [166, 74, 118], "isthing": 1, "id": 76, "name": "keyboard"},
-#     {"color": [219, 142, 185], "isthing": 1, "id": 77, "name": "cell phone"},
-#     {"color": [79, 210, 114], "isthing": 1, "id": 78, "name": "microwave"},
-#     {"color": [178, 90, 62], "isthing": 1, "id": 79, "name": "oven"},
-#     {"color": [65, 70, 15], "isthing": 1, "id": 80, "name": "toaster"},
-#     {"color": [127, 167, 115], "isthing": 1, "id": 81, "name": "sink"},
-#     {"color": [59, 105, 106], "isthing": 1, "id": 82, "name": "refrigerator"},
-#     {"color": [142, 108, 45], "isthing": 1, "id": 84, "name": "book"},
-#     {"color": [196, 172, 0], "isthing": 1, "id": 85, "name": "clock"},
-#     {"color": [95, 54, 80], "isthing": 1, "id": 86, "name": "vase"},
-#     {"color": [128, 76, 255], "isthing": 1, "id": 87, "name": "scissors"},
-#     {"color": [201, 57, 1], "isthing": 1, "id": 88, "name": "teddy bear"},
-#     {"color": [246, 0, 122], "isthing": 1, "id": 89, "name": "hair drier"},
-#     {"color": [191, 162, 208], "isthing": 1, "id": 90, "name": "toothbrush"},
-#     {"color": [255, 255, 128], "isthing": 0, "id": 92, "name": "banner"},
-#     {"color": [147, 211, 203], "isthing": 0, "id": 93, "name": "blanket"},
-#     {"color": [150, 100, 100], "isthing": 0, "id": 95, "name": "bridge"},
-#     {"color": [168, 171, 172], "isthing": 0, "id": 100, "name": "cardboard"},
-#     {"color": [146, 112, 198], "isthing": 0, "id": 107, "name": "counter"},
-#     {"color": [210, 170, 100], "isthing": 0, "id": 109, "name": "curtain"},
-#     {"color": [92, 136, 89], "isthing": 0, "id": 112, "name": "door-stuff"},
-#     {"color": [218, 88, 184], "isthing": 0, "id": 118, "name": "floor-wood"},
-#     {"color": [241, 129, 0], "isthing": 0, "id": 119, "name": "flower"},
-#     {"color": [217, 17, 255], "isthing": 0, "id": 122, "name": "fruit"},
-#     {"color": [124, 74, 181], "isthing": 0, "id": 125, "name": "gravel"},
-#     {"color": [70, 70, 70], "isthing": 0, "id": 128, "name": "house"},
-#     {"color": [255, 228, 255], "isthing": 0, "id": 130, "name": "light"},
-#     {"color": [154, 208, 0], "isthing": 0, "id": 133, "name": "mirror-stuff"},
-#     {"color": [193, 0, 92], "isthing": 0, "id": 138, "name": "net"},
-#     {"color": [76, 91, 113], "isthing": 0, "id": 141, "name": "pillow"},
-#     {"color": [255, 180, 195], "isthing": 0, "id": 144, "name": "platform"},
-#     {"color": [106, 154, 176], "isthing": 0, "id": 145, "name": "playingfield"},
-#     {"color": [230, 150, 140], "isthing": 0, "id": 147, "name": "railroad"},
-#     {"color": [60, 143, 255], "isthing": 0, "id": 148, "name": "river"},
-#     {"color": [128, 64, 128], "isthing": 0, "id": 149, "name": "road"},
-#     {"color": [92, 82, 55], "isthing": 0, "id": 151, "name": "roof"},
-#     {"color": [254, 212, 124], "isthing": 0, "id": 154, "name": "sand"},
-#     {"color": [73, 77, 174], "isthing": 0, "id": 155, "name": "sea"},
-#     {"color": [255, 160, 98], "isthing": 0, "id": 156, "name": "shelf"},
-#     {"color": [255, 255, 255], "isthing": 0, "id": 159, "name": "snow"},
-#     {"color": [104, 84, 109], "isthing": 0, "id": 161, "name": "stairs"},
-#     {"color": [169, 164, 131], "isthing": 0, "id": 166, "name": "tent"},
-#     {"color": [225, 199, 255], "isthing": 0, "id": 168, "name": "towel"},
-#     {"color": [137, 54, 74], "isthing": 0, "id": 171, "name": "wall-brick"},
-#     {"color": [135, 158, 223], "isthing": 0, "id": 175, "name": "wall-stone"},
-#     {"color": [7, 246, 231], "isthing": 0, "id": 176, "name": "wall-tile"},
-#     {"color": [107, 255, 200], "isthing": 0, "id": 177, "name": "wall-wood"},
-#     {"color": [58, 41, 149], "isthing": 0, "id": 178, "name": "water-other"},
-#     {"color": [183, 121, 142], "isthing": 0, "id": 180, "name": "window-blind"},
-#     {"color": [255, 73, 97], "isthing": 0, "id": 181, "name": "window-other"},
-#     {"color": [107, 142, 35], "isthing": 0, "id": 184, "name": "tree-merged"},
-#     {"color": [190, 153, 153], "isthing": 0, "id": 185, "name": "fence-merged"},
-#     {"color": [146, 139, 141], "isthing": 0, "id": 186, "name": "ceiling-merged"},
-#     {"color": [70, 130, 180], "isthing": 0, "id": 187, "name": "sky-other-merged"},
-#     {"color": [134, 199, 156], "isthing": 0, "id": 188, "name": "cabinet-merged"},
-#     {"color": [209, 226, 140], "isthing": 0, "id": 189, "name": "table-merged"},
-#     {"color": [96, 36, 108], "isthing": 0, "id": 190, "name": "floor-other-merged"},
-#     {"color": [96, 96, 96], "isthing": 0, "id": 191, "name": "pavement-merged"},
-#     {"color": [64, 170, 64], "isthing": 0, "id": 192, "name": "mountain-merged"},
-#     {"color": [152, 251, 152], "isthing": 0, "id": 193, "name": "grass-merged"},
-#     {"color": [208, 229, 228], "isthing": 0, "id": 194, "name": "dirt-merged"},
-#     {"color": [206, 186, 171], "isthing": 0, "id": 195, "name": "paper-merged"},
-#     {"color": [152, 161, 64], "isthing": 0, "id": 196, "name": "food-other-merged"},
-#     {"color": [116, 112, 0], "isthing": 0, "id": 197, "name": "building-other-merged"},
-#     {"color": [0, 114, 143], "isthing": 0, "id": 198, "name": "rock-merged"},
-#     {"color": [102, 102, 156], "isthing": 0, "id": 199, "name": "wall-other-merged"},
-#     {"color": [250, 141, 255], "isthing": 0, "id": 200, "name": "rug-merged"},
-# ]
-
-# COCO_CATEGORIES = [
-#     {"color": [220, 20, 60], "isthing": 1, "id": 1, "name": "text"},
-#     {"color": [250, 141, 255], "isthing": 0, "id": 2, "name": "background"},
-# ]
-
-# ocr catagories list
 COCO_CATEGORIES = [
-    {"color": [220, 20, 60], "isthing": 1, "id": 1, "name": "text"},
-    {"color": [119, 11, 32], "isthing": 1, "id": 2, "name": "0"},
-    {"color": [0, 0, 142], "isthing": 1, "id": 3, "name": "1"},
-    {"color": [0, 0, 230], "isthing": 1, "id": 4, "name": "2"},
-    {"color": [106, 0, 228], "isthing": 1, "id": 5, "name": "3"},
-    {"color": [0, 60, 100], "isthing": 1, "id": 6, "name": "4"},
-    {"color": [0, 80, 100], "isthing": 1, "id": 7, "name": "5"},
-    {"color": [0, 0, 70], "isthing": 1, "id": 8, "name": "6"},
-    {"color": [0, 0, 192], "isthing": 1, "id": 9, "name": "7"},
-    {"color": [250, 170, 30], "isthing": 1, "id": 10, "name": "8"},
-    {"color": [100, 170, 30], "isthing": 1, "id": 11, "name": "9"},
-    {"color": [220, 220, 0], "isthing": 1, "id": 12, "name": "A"},
-    {"color": [175, 116, 175], "isthing": 1, "id": 13, "name": "B"},
-    {"color": [250, 0, 30], "isthing": 1, "id": 14, "name": "C"},
-    {"color": [165, 42, 42], "isthing": 1, "id": 15, "name": "D"},
-    {"color": [255, 77, 255], "isthing": 1, "id": 16, "name": "E"},
-    {"color": [0, 226, 252], "isthing": 1, "id": 17, "name": "F"},
-    {"color": [182, 182, 255], "isthing": 1, "id": 18, "name": "G"},
-    {"color": [0, 82, 0], "isthing": 1, "id": 19, "name": "H"},
-    {"color": [120, 166, 157], "isthing": 1, "id": 20, "name": "I"},
-    {"color": [110, 76, 0], "isthing": 1, "id": 21, "name": "J"},
-    {"color": [174, 57, 255], "isthing": 1, "id": 22, "name": "K"},
-    {"color": [199, 100, 0], "isthing": 1, "id": 23, "name": "L"},
-    {"color": [72, 0, 118], "isthing": 1, "id": 24, "name": "M"},
-    {"color": [255, 179, 240], "isthing": 1, "id": 25, "name": "N"},
-    {"color": [0, 125, 92], "isthing": 1, "id": 26, "name": "O"},
-    {"color": [209, 0, 151], "isthing": 1, "id": 27, "name": "P"},
-    {"color": [188, 208, 182], "isthing": 1, "id": 28, "name": "Q"},
-    {"color": [0, 220, 176], "isthing": 1, "id": 29, "name": "R"},
-    {"color": [255, 99, 164], "isthing": 1, "id": 30, "name": "S"},
-    {"color": [92, 0, 73], "isthing": 1, "id": 31, "name": "T"},
-    {"color": [133, 129, 255], "isthing": 1, "id": 32, "name": "U"},
-    {"color": [78, 180, 255], "isthing": 1, "id": 33, "name": "V"},
-    {"color": [0, 228, 0], "isthing": 1, "id": 34, "name": "W"},
-    {"color": [174, 255, 243], "isthing": 1, "id": 35, "name": "X"},
-    {"color": [45, 89, 255], "isthing": 1, "id": 36, "name": "Y"},
-    {"color": [134, 134, 103], "isthing": 1, "id": 37, "name": "Z"},
-    {"color": [145, 148, 174], "isthing": 1, "id": 38, "name": "a"},
-    {"color": [255, 208, 186], "isthing": 1, "id": 39, "name": "b"},
-    {"color": [197, 226, 255], "isthing": 1, "id": 40, "name": "c"},
-    {"color": [171, 134, 1], "isthing": 1, "id": 41, "name": "d"},
-    {"color": [109, 63, 54], "isthing": 1, "id": 42, "name": "e"},
-    {"color": [207, 138, 255], "isthing": 1, "id": 43, "name": "f"},
-    {"color": [151, 0, 95], "isthing": 1, "id": 44, "name": "g"},
-    {"color": [9, 80, 61], "isthing": 1, "id": 45, "name": "h"},
-    {"color": [84, 105, 51], "isthing": 1, "id": 46, "name": "i"},
-    {"color": [74, 65, 105], "isthing": 1, "id": 47, "name": "j"},
-    {"color": [166, 196, 102], "isthing": 1, "id": 48, "name": "k"},
-    {"color": [208, 195, 210], "isthing": 1, "id": 49, "name": "l"},
-    {"color": [255, 109, 65], "isthing": 1, "id": 50, "name": "m"},
-    {"color": [0, 143, 149], "isthing": 1, "id": 51, "name": "n"},
-    {"color": [179, 0, 194], "isthing": 1, "id": 52, "name": "o"},
-    {"color": [209, 99, 106], "isthing": 1, "id": 53, "name": "p"},
-    {"color": [5, 121, 0], "isthing": 1, "id": 54, "name": "q"},
-    {"color": [227, 255, 205], "isthing": 1, "id": 55, "name": "r"},
-    {"color": [147, 186, 208], "isthing": 1, "id": 56, "name": "s"},
-    {"color": [153, 69, 1], "isthing": 1, "id": 57, "name": "t"},
-    {"color": [3, 95, 161], "isthing": 1, "id": 58, "name": "u"},
-    {"color": [163, 255, 0], "isthing": 1, "id": 59, "name": "v"},
-    {"color": [119, 0, 170], "isthing": 1, "id": 60, "name": "w"},
-    {"color": [0, 182, 199], "isthing": 1, "id": 61, "name": "x"},
-    {"color": [0, 165, 120], "isthing": 1, "id": 62, "name": "y"},
-    {"color": [183, 130, 88], "isthing": 1, "id": 63, "name": "z"},
-    {"color": [250, 141, 255], "isthing": 0, "id": 64, "name": "background"},
+    {"color": [220, 20, 60], "isthing": 1, "id": 1, "name": "person"},
+    {"color": [119, 11, 32], "isthing": 1, "id": 2, "name": "bicycle"},
+    {"color": [0, 0, 142], "isthing": 1, "id": 3, "name": "car"},
+    {"color": [0, 0, 230], "isthing": 1, "id": 4, "name": "motorcycle"},
+    {"color": [106, 0, 228], "isthing": 1, "id": 5, "name": "airplane"},
+    {"color": [0, 60, 100], "isthing": 1, "id": 6, "name": "bus"},
+    {"color": [0, 80, 100], "isthing": 1, "id": 7, "name": "train"},
+    {"color": [0, 0, 70], "isthing": 1, "id": 8, "name": "truck"},
+    {"color": [0, 0, 192], "isthing": 1, "id": 9, "name": "boat"},
+    {"color": [250, 170, 30], "isthing": 1, "id": 10, "name": "traffic light"},
+    {"color": [100, 170, 30], "isthing": 1, "id": 11, "name": "fire hydrant"},
+    {"color": [220, 220, 0], "isthing": 1, "id": 13, "name": "stop sign"},
+    {"color": [175, 116, 175], "isthing": 1, "id": 14, "name": "parking meter"},
+    {"color": [250, 0, 30], "isthing": 1, "id": 15, "name": "bench"},
+    {"color": [165, 42, 42], "isthing": 1, "id": 16, "name": "bird"},
+    {"color": [255, 77, 255], "isthing": 1, "id": 17, "name": "cat"},
+    {"color": [0, 226, 252], "isthing": 1, "id": 18, "name": "dog"},
+    {"color": [182, 182, 255], "isthing": 1, "id": 19, "name": "horse"},
+    {"color": [0, 82, 0], "isthing": 1, "id": 20, "name": "sheep"},
+    {"color": [120, 166, 157], "isthing": 1, "id": 21, "name": "cow"},
+    {"color": [110, 76, 0], "isthing": 1, "id": 22, "name": "elephant"},
+    {"color": [174, 57, 255], "isthing": 1, "id": 23, "name": "bear"},
+    {"color": [199, 100, 0], "isthing": 1, "id": 24, "name": "zebra"},
+    {"color": [72, 0, 118], "isthing": 1, "id": 25, "name": "giraffe"},
+    {"color": [255, 179, 240], "isthing": 1, "id": 27, "name": "backpack"},
+    {"color": [0, 125, 92], "isthing": 1, "id": 28, "name": "umbrella"},
+    {"color": [209, 0, 151], "isthing": 1, "id": 31, "name": "handbag"},
+    {"color": [188, 208, 182], "isthing": 1, "id": 32, "name": "tie"},
+    {"color": [0, 220, 176], "isthing": 1, "id": 33, "name": "suitcase"},
+    {"color": [255, 99, 164], "isthing": 1, "id": 34, "name": "frisbee"},
+    {"color": [92, 0, 73], "isthing": 1, "id": 35, "name": "skis"},
+    {"color": [133, 129, 255], "isthing": 1, "id": 36, "name": "snowboard"},
+    {"color": [78, 180, 255], "isthing": 1, "id": 37, "name": "sports ball"},
+    {"color": [0, 228, 0], "isthing": 1, "id": 38, "name": "kite"},
+    {"color": [174, 255, 243], "isthing": 1, "id": 39, "name": "baseball bat"},
+    {"color": [45, 89, 255], "isthing": 1, "id": 40, "name": "baseball glove"},
+    {"color": [134, 134, 103], "isthing": 1, "id": 41, "name": "skateboard"},
+    {"color": [145, 148, 174], "isthing": 1, "id": 42, "name": "surfboard"},
+    {"color": [255, 208, 186], "isthing": 1, "id": 43, "name": "tennis racket"},
+    {"color": [197, 226, 255], "isthing": 1, "id": 44, "name": "bottle"},
+    {"color": [171, 134, 1], "isthing": 1, "id": 46, "name": "wine glass"},
+    {"color": [109, 63, 54], "isthing": 1, "id": 47, "name": "cup"},
+    {"color": [207, 138, 255], "isthing": 1, "id": 48, "name": "fork"},
+    {"color": [151, 0, 95], "isthing": 1, "id": 49, "name": "knife"},
+    {"color": [9, 80, 61], "isthing": 1, "id": 50, "name": "spoon"},
+    {"color": [84, 105, 51], "isthing": 1, "id": 51, "name": "bowl"},
+    {"color": [74, 65, 105], "isthing": 1, "id": 52, "name": "banana"},
+    {"color": [166, 196, 102], "isthing": 1, "id": 53, "name": "apple"},
+    {"color": [208, 195, 210], "isthing": 1, "id": 54, "name": "sandwich"},
+    {"color": [255, 109, 65], "isthing": 1, "id": 55, "name": "orange"},
+    {"color": [0, 143, 149], "isthing": 1, "id": 56, "name": "broccoli"},
+    {"color": [179, 0, 194], "isthing": 1, "id": 57, "name": "carrot"},
+    {"color": [209, 99, 106], "isthing": 1, "id": 58, "name": "hot dog"},
+    {"color": [5, 121, 0], "isthing": 1, "id": 59, "name": "pizza"},
+    {"color": [227, 255, 205], "isthing": 1, "id": 60, "name": "donut"},
+    {"color": [147, 186, 208], "isthing": 1, "id": 61, "name": "cake"},
+    {"color": [153, 69, 1], "isthing": 1, "id": 62, "name": "chair"},
+    {"color": [3, 95, 161], "isthing": 1, "id": 63, "name": "couch"},
+    {"color": [163, 255, 0], "isthing": 1, "id": 64, "name": "potted plant"},
+    {"color": [119, 0, 170], "isthing": 1, "id": 65, "name": "bed"},
+    {"color": [0, 182, 199], "isthing": 1, "id": 67, "name": "dining table"},
+    {"color": [0, 165, 120], "isthing": 1, "id": 70, "name": "toilet"},
+    {"color": [183, 130, 88], "isthing": 1, "id": 72, "name": "tv"},
+    {"color": [95, 32, 0], "isthing": 1, "id": 73, "name": "laptop"},
+    {"color": [130, 114, 135], "isthing": 1, "id": 74, "name": "mouse"},
+    {"color": [110, 129, 133], "isthing": 1, "id": 75, "name": "remote"},
+    {"color": [166, 74, 118], "isthing": 1, "id": 76, "name": "keyboard"},
+    {"color": [219, 142, 185], "isthing": 1, "id": 77, "name": "cell phone"},
+    {"color": [79, 210, 114], "isthing": 1, "id": 78, "name": "microwave"},
+    {"color": [178, 90, 62], "isthing": 1, "id": 79, "name": "oven"},
+    {"color": [65, 70, 15], "isthing": 1, "id": 80, "name": "toaster"},
+    {"color": [127, 167, 115], "isthing": 1, "id": 81, "name": "sink"},
+    {"color": [59, 105, 106], "isthing": 1, "id": 82, "name": "refrigerator"},
+    {"color": [142, 108, 45], "isthing": 1, "id": 84, "name": "book"},
+    {"color": [196, 172, 0], "isthing": 1, "id": 85, "name": "clock"},
+    {"color": [95, 54, 80], "isthing": 1, "id": 86, "name": "vase"},
+    {"color": [128, 76, 255], "isthing": 1, "id": 87, "name": "scissors"},
+    {"color": [201, 57, 1], "isthing": 1, "id": 88, "name": "teddy bear"},
+    {"color": [246, 0, 122], "isthing": 1, "id": 89, "name": "hair drier"},
+    {"color": [191, 162, 208], "isthing": 1, "id": 90, "name": "toothbrush"},
+    {"color": [255, 255, 128], "isthing": 0, "id": 92, "name": "banner"},
+    {"color": [147, 211, 203], "isthing": 0, "id": 93, "name": "blanket"},
+    {"color": [150, 100, 100], "isthing": 0, "id": 95, "name": "bridge"},
+    {"color": [168, 171, 172], "isthing": 0, "id": 100, "name": "cardboard"},
+    {"color": [146, 112, 198], "isthing": 0, "id": 107, "name": "counter"},
+    {"color": [210, 170, 100], "isthing": 0, "id": 109, "name": "curtain"},
+    {"color": [92, 136, 89], "isthing": 0, "id": 112, "name": "door-stuff"},
+    {"color": [218, 88, 184], "isthing": 0, "id": 118, "name": "floor-wood"},
+    {"color": [241, 129, 0], "isthing": 0, "id": 119, "name": "flower"},
+    {"color": [217, 17, 255], "isthing": 0, "id": 122, "name": "fruit"},
+    {"color": [124, 74, 181], "isthing": 0, "id": 125, "name": "gravel"},
+    {"color": [70, 70, 70], "isthing": 0, "id": 128, "name": "house"},
+    {"color": [255, 228, 255], "isthing": 0, "id": 130, "name": "light"},
+    {"color": [154, 208, 0], "isthing": 0, "id": 133, "name": "mirror-stuff"},
+    {"color": [193, 0, 92], "isthing": 0, "id": 138, "name": "net"},
+    {"color": [76, 91, 113], "isthing": 0, "id": 141, "name": "pillow"},
+    {"color": [255, 180, 195], "isthing": 0, "id": 144, "name": "platform"},
+    {"color": [106, 154, 176], "isthing": 0, "id": 145, "name": "playingfield"},
+    {"color": [230, 150, 140], "isthing": 0, "id": 147, "name": "railroad"},
+    {"color": [60, 143, 255], "isthing": 0, "id": 148, "name": "river"},
+    {"color": [128, 64, 128], "isthing": 0, "id": 149, "name": "road"},
+    {"color": [92, 82, 55], "isthing": 0, "id": 151, "name": "roof"},
+    {"color": [254, 212, 124], "isthing": 0, "id": 154, "name": "sand"},
+    {"color": [73, 77, 174], "isthing": 0, "id": 155, "name": "sea"},
+    {"color": [255, 160, 98], "isthing": 0, "id": 156, "name": "shelf"},
+    {"color": [255, 255, 255], "isthing": 0, "id": 159, "name": "snow"},
+    {"color": [104, 84, 109], "isthing": 0, "id": 161, "name": "stairs"},
+    {"color": [169, 164, 131], "isthing": 0, "id": 166, "name": "tent"},
+    {"color": [225, 199, 255], "isthing": 0, "id": 168, "name": "towel"},
+    {"color": [137, 54, 74], "isthing": 0, "id": 171, "name": "wall-brick"},
+    {"color": [135, 158, 223], "isthing": 0, "id": 175, "name": "wall-stone"},
+    {"color": [7, 246, 231], "isthing": 0, "id": 176, "name": "wall-tile"},
+    {"color": [107, 255, 200], "isthing": 0, "id": 177, "name": "wall-wood"},
+    {"color": [58, 41, 149], "isthing": 0, "id": 178, "name": "water-other"},
+    {"color": [183, 121, 142], "isthing": 0, "id": 180, "name": "window-blind"},
+    {"color": [255, 73, 97], "isthing": 0, "id": 181, "name": "window-other"},
+    {"color": [107, 142, 35], "isthing": 0, "id": 184, "name": "tree-merged"},
+    {"color": [190, 153, 153], "isthing": 0, "id": 185, "name": "fence-merged"},
+    {"color": [146, 139, 141], "isthing": 0, "id": 186, "name": "ceiling-merged"},
+    {"color": [70, 130, 180], "isthing": 0, "id": 187, "name": "sky-other-merged"},
+    {"color": [134, 199, 156], "isthing": 0, "id": 188, "name": "cabinet-merged"},
+    {"color": [209, 226, 140], "isthing": 0, "id": 189, "name": "table-merged"},
+    {"color": [96, 36, 108], "isthing": 0, "id": 190, "name": "floor-other-merged"},
+    {"color": [96, 96, 96], "isthing": 0, "id": 191, "name": "pavement-merged"},
+    {"color": [64, 170, 64], "isthing": 0, "id": 192, "name": "mountain-merged"},
+    {"color": [152, 251, 152], "isthing": 0, "id": 193, "name": "grass-merged"},
+    {"color": [208, 229, 228], "isthing": 0, "id": 194, "name": "dirt-merged"},
+    {"color": [206, 186, 171], "isthing": 0, "id": 195, "name": "paper-merged"},
+    {"color": [152, 161, 64], "isthing": 0, "id": 196, "name": "food-other-merged"},
+    {"color": [116, 112, 0], "isthing": 0, "id": 197, "name": "building-other-merged"},
+    {"color": [0, 114, 143], "isthing": 0, "id": 198, "name": "rock-merged"},
+    {"color": [102, 102, 156], "isthing": 0, "id": 199, "name": "wall-other-merged"},
+    {"color": [250, 141, 255], "isthing": 0, "id": 200, "name": "rug-merged"},
 ]
 
-
 # fmt: off
 COCO_PERSON_KEYPOINT_NAMES = (
     "nose",
@@ -265,7 +191,7 @@ KEYPOINT_CONNECTION_RULES = [
 def _get_coco_instances_meta():
     thing_ids = [k["id"] for k in COCO_CATEGORIES if k["isthing"] == 1]
     thing_colors = [k["color"] for k in COCO_CATEGORIES if k["isthing"] == 1]
-    assert len(thing_ids) == 63, len(thing_ids)
+    assert len(thing_ids) == 80, len(thing_ids)
     # Mapping from the incontiguous COCO category id to an id in [0, 79]
     thing_dataset_id_to_contiguous_id = {k: i for i, k in enumerate(thing_ids)}
     thing_classes = [k["name"] for k in COCO_CATEGORIES if k["isthing"] == 1]
@@ -282,7 +208,7 @@ def _get_coco_panoptic_separated_meta():
     Returns metadata for "separated" version of the panoptic segmentation dataset.
     """
     stuff_ids = [k["id"] for k in COCO_CATEGORIES if k["isthing"] == 0]
-    assert len(stuff_ids) == 1, len(stuff_ids)
+    assert len(stuff_ids) == 53, len(stuff_ids)
 
     # For semantic segmentation, this mapping maps from contiguous stuff id
     # (in [0, 53], used in models) to ids in the dataset (used for processing results)
diff --git a/detectron2/data/detection_utils.py b/detectron2/data/detection_utils.py
index 46a5459..c614dae 100644
--- a/detectron2/data/detection_utils.py
+++ b/detectron2/data/detection_utils.py
@@ -6,7 +6,6 @@ Common data processing utilities that are used in a
 typical object detection data pipeline.
 """
 import logging
-import random
 import numpy as np
 import torch
 from fvcore.common.file_io import PathManager
@@ -408,9 +407,5 @@ def build_transform_gen(cfg, is_train):
     tfm_gens.append(T.ResizeShortestEdge(min_size, max_size, sample_style))
     if is_train:
         tfm_gens.append(T.RandomFlip())
-        tfm_gens.append(T.RandomContrast(0.5,1.5))
-        tfm_gens.append(T.RandomBrightness(0.5,1.5))
-        tfm_gens.append(T.RandomSaturation(0.5,1.5))
-        tfm_gens.append(T.RandomLighting(random.random()+0.5))
         logger.info("TransformGens used in training: " + str(tfm_gens))
     return tfm_gens
diff --git a/detectron2/engine/defaults.py b/detectron2/engine/defaults.py
index 00ab43c..dc92224 100644
--- a/detectron2/engine/defaults.py
+++ b/detectron2/engine/defaults.py
@@ -56,7 +56,6 @@ def default_argument_parser():
     parser.add_argument("--config-file", default="", metavar="FILE", help="path to config file")
     parser.add_argument(
         "--resume",
-        default=True,
         action="store_true",
         help="whether to attempt to resume from the checkpoint directory",
     )
@@ -152,8 +151,7 @@ class DefaultPredictor:
         self.metadata = MetadataCatalog.get(cfg.DATASETS.TEST[0])
 
         checkpointer = DetectionCheckpointer(self.model)
-        # False : whether from last_checkpoint
-        checkpointer.load(cfg.MODEL.WEIGHTS,False)
+        checkpointer.load(cfg.MODEL.WEIGHTS)
 
         self.transform_gen = T.ResizeShortestEdge(
             [cfg.INPUT.MIN_SIZE_TEST, cfg.INPUT.MIN_SIZE_TEST], cfg.INPUT.MAX_SIZE_TEST
@@ -231,7 +229,7 @@ class DefaultTrainer(SimpleTrainer):
         # For training, wrap with DDP. But don't need this for inference.
         if comm.get_world_size() > 1:
             model = DistributedDataParallel(
-                model, device_ids=[comm.get_local_rank()], broadcast_buffers=False,find_unused_parameters=True
+                model, device_ids=[comm.get_local_rank()], broadcast_buffers=False
             )
         super().__init__(model, data_loader, optimizer)
 
diff --git a/detectron2/modeling/roi_heads/fast_rcnn.py b/detectron2/modeling/roi_heads/fast_rcnn.py
index 5478250..1342edf 100644
--- a/detectron2/modeling/roi_heads/fast_rcnn.py
+++ b/detectron2/modeling/roi_heads/fast_rcnn.py
@@ -105,8 +105,8 @@ def fast_rcnn_inference_single_image(
         boxes = boxes[filter_mask]
     scores = scores[filter_mask]
 
+    # Apply per-class NMS
     keep = batched_nms(boxes, scores, filter_inds[:, 1], nms_thresh)
-
     if topk_per_image >= 0:
         keep = keep[:topk_per_image]
     boxes, scores, filter_inds = boxes[keep], scores[keep], filter_inds[keep]
diff --git a/detectron2/modeling/roi_heads/roi_heads.py b/detectron2/modeling/roi_heads/roi_heads.py
index 422f368..7c9ab4d 100644
--- a/detectron2/modeling/roi_heads/roi_heads.py
+++ b/detectron2/modeling/roi_heads/roi_heads.py
@@ -1,6 +1,5 @@
 # Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
 import logging
-import copy
 import numpy as np
 from typing import Dict
 import torch
@@ -21,9 +20,6 @@ from .box_head import build_box_head
 from .fast_rcnn import FastRCNNOutputLayers, FastRCNNOutputs
 from .keypoint_head import build_keypoint_head, keypoint_rcnn_inference, keypoint_rcnn_loss
 from .mask_head import build_mask_head, mask_rcnn_inference, mask_rcnn_loss
-from .seg_head import build_seg_head, build_seg_head_loss
-from .mutil_path_fuse_module import build_mutil_path_fuse_module
-
 
 ROI_HEADS_REGISTRY = Registry("ROI_HEADS")
 ROI_HEADS_REGISTRY.__doc__ = """
@@ -242,7 +238,6 @@ class ROIHeads(torch.nn.Module):
                 targets_per_image.gt_boxes, proposals_per_image.proposal_boxes
             )
             matched_idxs, matched_labels = self.proposal_matcher(match_quality_matrix)
-
             sampled_idxs, gt_classes = self._sample_proposals(
                 matched_idxs, matched_labels, targets_per_image.gt_classes
             )
@@ -472,12 +467,6 @@ class StandardROIHeads(ROIHeads):
         self._init_mask_head(cfg)
         self._init_keypoint_head(cfg)
 
-        self.mutil_path_fuse_on = cfg.MODEL.TEXTFUSENET_MUTIL_PATH_FUSE_ON
-        if self.mutil_path_fuse_on:
-            self._init_seg_head(cfg)
-            self._init_mutil_path_fuse_module(cfg)
-
-
     def _init_box_head(self, cfg):
         # fmt: off
         pooler_resolution = cfg.MODEL.ROI_BOX_HEAD.POOLER_RESOLUTION
@@ -557,31 +546,22 @@ class StandardROIHeads(ROIHeads):
             cfg, ShapeSpec(channels=in_channels, width=pooler_resolution, height=pooler_resolution)
         )
 
-    def _init_seg_head(self, cfg):
-        self.fpn_features_fused_level = cfg.MODEL.TEXTFUSENET_SEG_HEAD.FPN_FEATURES_FUSED_LEVEL
-        self.seg_head = build_seg_head(cfg)
-
-    def _init_mutil_path_fuse_module(self, cfg):
-        self.mutil_path_fuse_module = build_mutil_path_fuse_module(cfg)
-
-
     def forward(self, images, features, proposals, targets=None):
         """
         See :class:`ROIHeads.forward`.
         """
-
-        features_list = [features[f] for f in self.in_features]
-
+        del images
         if self.training:
-            del images
             proposals = self.label_and_sample_proposals(proposals, targets)
+        del targets
 
+        features_list = [features[f] for f in self.in_features]
 
         if self.training:
             losses = self._forward_box(features_list, proposals)
             # During training the proposals used by the box head are
             # used by the mask, keypoint (and densepose) heads.
-            losses.update(self._forward_mask(features_list, proposals, targets))
+            losses.update(self._forward_mask(features_list, proposals))
             losses.update(self._forward_keypoint(features_list, proposals))
             return proposals, losses
         else:
@@ -632,13 +612,11 @@ class StandardROIHeads(ROIHeads):
             In training, a dict of losses.
             In inference, a list of `Instances`, the predicted instances.
         """
-
         box_features = self.box_pooler(features, [x.proposal_boxes for x in proposals])
         box_features = self.box_head(box_features)
         pred_class_logits, pred_proposal_deltas = self.box_predictor(box_features)
         del box_features
 
-
         outputs = FastRCNNOutputs(
             self.box2box_transform,
             pred_class_logits,
@@ -654,7 +632,7 @@ class StandardROIHeads(ROIHeads):
             )
             return pred_instances
 
-    def _forward_mask(self, features, instances,targets=None):
+    def _forward_mask(self, features, instances):
         """
         Forward logic of the mask prediction branch.
 
@@ -676,35 +654,11 @@ class StandardROIHeads(ROIHeads):
             proposals, _ = select_foreground_proposals(instances, self.num_classes)
             proposal_boxes = [x.proposal_boxes for x in proposals]
             mask_features = self.mask_pooler(features, proposal_boxes)
-
-            ###############################################  mutil_path_fuse  ###################################################
-            if self.mutil_path_fuse_on:
-                image_shape = [proposals[0].image_size[1], proposals[0].image_size[0]]
-                seg_logits, global_context = self.seg_head(features, self.fpn_features_fused_level, proposal_boxes, image_shape)
-                mask_features = self.mutil_path_fuse_module(mask_features, global_context, proposals)
-            #####################################################################################################################
-
             mask_logits = self.mask_head(mask_features)
-
-            if self.mutil_path_fuse_on:
-                seg_loss = build_seg_head_loss()
-                return {
-                        "loss_mask": mask_rcnn_loss(mask_logits, proposals),
-                        "loss_seg": seg_loss(seg_logits, targets),
-                }
-            else:
-                return {"loss_mask": mask_rcnn_loss(mask_logits, proposals)}
+            return {"loss_mask": mask_rcnn_loss(mask_logits, proposals)}
         else:
             pred_boxes = [x.pred_boxes for x in instances]
             mask_features = self.mask_pooler(features, pred_boxes)
-
-            ###############################################  mutil_path_fuse  ###################################################
-            if self.mutil_path_fuse_on:
-                image_shape = [instances[0].image_size[1], instances[0].image_size[0]]
-                seg_logits,global_context = self.seg_head(features, self.fpn_features_fused_level, pred_boxes, image_shape)
-                mask_features = self.mutil_path_fuse_module(mask_features, global_context, instances)
-            ######################################################################################################################
-
             mask_logits = self.mask_head(mask_features)
             mask_rcnn_inference(mask_logits, instances)
             return instances
diff --git a/detectron2/utils/visualizer.py b/detectron2/utils/visualizer.py
index 12f2652..b2e97a2 100644
--- a/detectron2/utils/visualizer.py
+++ b/detectron2/utils/visualizer.py
@@ -359,13 +359,6 @@ class Visualizer:
             )
             alpha = 0.3
 
-
-        ### get polygons. yejian 2020-5-6
-        polygon_list = []
-        for mask in masks:
-            polygon_list.append(mask.polygons)
-
-
         self.overlay_instances(
             masks=masks,
             boxes=boxes,
@@ -374,7 +367,7 @@ class Visualizer:
             assigned_colors=colors,
             alpha=alpha,
         )
-        return self.output, polygon_list
+        return self.output
 
     def draw_sem_seg(self, sem_seg, area_threshold=None, alpha=0.8):
         """
diff --git a/docker/Dockerfile b/docker/Dockerfile
index 29dbd3e..b8cb3f4 100644
--- a/docker/Dockerfile
+++ b/docker/Dockerfile
@@ -1,59 +1,43 @@
-# Demo docker, not tested for production
-
-# Inspired by
-# Detectron2 Dockerfile
-# https://github.com/facebookresearch/detectron2/blob/master/docker/Dockerfile
-
-# TextFuseNet step-by-step installation
-# https://github.com/ying09/TextFuseNet/blob/master/step-by-step%20installation.txt 
-
-# Docker conda installation
-# https://stackoverflow.com/a/62674910/6760875
-
 FROM nvidia/cuda:10.1-cudnn7-devel
 
-# install conda
-ENV PATH="/root/miniconda3/bin:${PATH}"
-ARG PATH="/root/miniconda3/bin:${PATH}"
-
-RUN apt-get update && apt-get install -y wget git libgl1-mesa-glx libglib2.0-0 && rm -rf /var/lib/apt/lists/*
-
-RUN wget \
-    https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh \
-    && mkdir /root/.conda \
-    && bash Miniconda3-latest-Linux-x86_64.sh -b \
-    && rm -f Miniconda3-latest-Linux-x86_64.sh 
-RUN conda --version
-
-# create environment
-RUN conda create --name textfusenet python=3.7.3
-
-# activate environment
-SHELL ["conda", "run", "-n", "textfusenet", "/bin/bash", "-c"]
-
-# force enable cuda
+ENV DEBIAN_FRONTEND noninteractive
+RUN apt-get update && apt-get install -y \
+	python3-opencv ca-certificates python3-dev git wget sudo && \
+  rm -rf /var/lib/apt/lists/*
+
+# create a non-root user
+ARG USER_ID=1000
+RUN useradd -m --no-log-init --system  --uid ${USER_ID} appuser -g sudo
+RUN echo '%sudo ALL=(ALL) NOPASSWD:ALL' >> /etc/sudoers
+USER appuser
+WORKDIR /home/appuser
+
+ENV PATH="/home/appuser/.local/bin:${PATH}"
+RUN wget https://bootstrap.pypa.io/get-pip.py && \
+	python3 get-pip.py --user && \
+	rm get-pip.py
+
+# install dependencies
+# See https://pytorch.org/ for other options if you use a different version of CUDA
+RUN pip install --user torch torchvision tensorboard cython
+RUN pip install --user 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'
+
+RUN pip install --user 'git+https://github.com/facebookresearch/fvcore'
+# install detectron2
+RUN git clone https://github.com/facebookresearch/detectron2 detectron2_repo
 ENV FORCE_CUDA="1"
-ARG TORCH_CUDA_ARCH_LIST="Kepler;Kepler+Tesla;Maxwell;Maxwell+Tegra;Pascal;Volta;Turing"
-ENV TORCH_CUDA_ARCH_LIST="${TORCH_CUDA_ARCH_LIST}"
-
-# install packages
-RUN conda install pytorch=1.3.1 torchvision cudatoolkit=10.1 -c pytorch
-RUN pip install opencv-python tensorboard yacs tqdm termcolor tabulate matplotlib cloudpickle wheel pycocotools
-
-# clone TextFuseNet
-RUN git clone https://github.com/ying09/TextFuseNet.git
-
-# set the working directory
-WORKDIR TextFuseNet
-
-# install fvcore
-RUN pip install fvcore-master.zip
-
-# build TextFuseNet
-RUN python setup.py build develop
-
-# activate environment for the user
-RUN echo "source activate textfusenet" > ~/.bashrc
-
-# use demo script as an entry point
-ENTRYPOINT python demo/${DEMO_FILE} --weights model.pth --output ./output_images/
\ No newline at end of file
+# This will build detectron2 for all common cuda architectures and take a lot more time,
+# because inside `docker build`, there is no way to tell which architecture will be used.
+ENV TORCH_CUDA_ARCH_LIST="Kepler;Kepler+Tesla;Maxwell;Maxwell+Tegra;Pascal;Volta;Turing"
+RUN pip install --user -e detectron2_repo
+
+# Set a fixed model cache directory.
+ENV FVCORE_CACHE="/tmp"
+WORKDIR /home/appuser/detectron2_repo
+
+# run it, for example:
+# wget http://images.cocodataset.org/val2017/000000439715.jpg -O input.jpg
+# python3 demo/demo.py  \
+	#--config-file configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml \
+	#--input input.jpg --output outputs/ \
+	#--opts MODEL.WEIGHTS detectron2://COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl
diff --git a/docker/README.md b/docker/README.md
index 9674f95..530b1b3 100644
--- a/docker/README.md
+++ b/docker/README.md
@@ -1,16 +1,24 @@
-# TextFuseNet Docker
-## Requirements
-- docker with Nvidia GPU Support
-- docker-compose
+## Run the container
+Change to the *docker* directory of this repository:
+```
+cd docker
+USER_ID=$UID docker-compose run detectron2
+```
 
-## Config
-You can configure the paths and models in the [.env](.env) file.
-I would recommend trying it with the default settings before.
+#### Using a persistent cache directory
+Prevents models to be re-downloaded on every run, by storing them in a cache directory.
 
-## Installation
-1. Download models as described in [here](models/README.md) (Default: model_ic15_r101.pth).
-2. Add some image files to the [input_images](../input_images) folder. (JPG format)
-3. Run
-```docker-compose up --force-recreate``` in this directory.
-   If you runned into the 'GPG error' problem in the 'RUN apt-get update' step of the Dockerfile, you can replace the Dockerfile contents with the commands in [Alternate.txt](Alternate.txt).
-4. You can find the processed images the [output_images](output_images) directory.
+`docker-compose run --volume=/path/to/cache:/tmp:rw detectron2`
+
+## Rebuild the container
+Rebuild the container  by `USER_ID=$UID docker-compose build detectron2`.
+This is only necessary when `Dockerfile` has been changed. The initial build is done automatically.
+
+## Install new dependencies
+Add the following to `Dockerfile` to make persistent changes.
+```
+RUN sudo apt-get update && sudo apt-get install -y \
+  nano vim emacs
+RUN pip install --user pandas
+```
+Or run them in the container to make temporary changes.
diff --git a/docker/docker-compose.yml b/docker/docker-compose.yml
index dda19db..e660f44 100644
--- a/docker/docker-compose.yml
+++ b/docker/docker-compose.yml
@@ -1,14 +1,18 @@
-# Demo docker, not tested for production
-version: "3.8"
+version: "2.3"
 services:
-  textfusenet:
-    environment:
-        DEMO_FILE: ${DEMO_FILE}
-    image: text-fuse-net:1.0.0
+  detectron2:
     build:
       context: .
+      dockerfile: Dockerfile
+      args:
+        USER_ID: ${USER_ID:-1000}
+    runtime: nvidia  # TODO: Exchange with "gpu: all" in the future (see https://github.com/facebookresearch/detectron2/pull/197/commits/00545e1f376918db4a8ce264d427a07c1e896c5a).
+    shm_size: "8gb"
+    ulimits:
+      memlock: -1
+      stack: 67108864
     volumes:
-      - "${MODEL_FILE}:/TextFuseNet/model.pth:ro"
-      - "${INPUT_IMAGE_FOLDER}:/TextFuseNet/input_images:ro"
-      - "${OUTPUT_IMAGE_FOLDER}:/TextFuseNet/output_images"
-    runtime: nvidia
\ No newline at end of file
+      - /tmp/.X11-unix:/tmp/.X11-unix:ro
+    environment:
+      - DISPLAY=$DISPLAY
+      - NVIDIA_VISIBLE_DEVICES=all
diff --git a/docs/tutorials/getting_started.md b/docs/tutorials/getting_started.md
deleted file mode 100644
index 683e622..0000000
--- a/docs/tutorials/getting_started.md
+++ /dev/null
@@ -1,79 +0,0 @@
-
-## Getting Started with Detectron2
-
-This document provides a brief intro of the usage of builtin command-line tools in detectron2.
-
-For a tutorial that involves actual coding with the API,
-see our [Colab Notebook](https://colab.research.google.com/drive/16jcaJoc6bCFAQ96jDe2HwtXj7BMD_-m5)
-which covers how to run inference with an
-existing model, and how to train a builtin model on a custom dataset.
-
-For more advanced tutorials, refer to our [documentation](https://detectron2.readthedocs.io/tutorials/extend.html).
-
-
-### Inference with Pre-trained Models
-
-1. Pick a model and its config file from
-	[model zoo](https://github.com/facebookresearch/detectron2/blob/master/MODEL_ZOO.md),
-	for example, `mask_rcnn_R_50_FPN_3x.yaml`.
-2. Run the demo with
-```
-python demo/demo.py --config-file configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml \
-  --input input1.jpg input2.jpg \
-	[--other-options]
-  --opts MODEL.WEIGHTS detectron2://COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl
-```
-The configs are made for training, therefore we need to specify `MODEL.WEIGHTS` to a model from model zoo for evaluation.
-This command will run the inference and show visualizations in an OpenCV window.
-
-For details of the command line arguments, see `demo.py -h`. Some common ones are:
-* To run __on your webcam__, replace `--input files` with `--webcam`.
-* To run __on a video__, replace `--input files` with `--video-input video.mp4`.
-* To run __on cpu__, add `MODEL.DEVICE cpu` after `--opts`.
-* To save outputs to a directory (for images) or a file (for webcam or video), use `--output`.
-
-
-### Use Detectron2 in Command Line
-
-We provide a script in "tools/train_net.py", that is made to train
-all the configs provided in detectron2.
-You may want to use it as a reference to write your own training script for a new research.
-
-To train a model with "train_net.py", first
-setup the corresponding datasets following
-[datasets/README.md](https://github.com/facebookresearch/detectron2/blob/master/datasets/README.md),
-then run:
-```
-python tools/train_net.py --num-gpus 8 \
-	--config-file configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_1x.yaml
-```
-
-The configs are made for 8-GPU training. To train on 1 GPU, change the batch size with:
-```
-python tools/train_net.py \
-	--config-file configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_1x.yaml \
-	SOLVER.IMS_PER_BATCH 2 SOLVER.BASE_LR 0.0025
-```
-
-For most models, CPU training is not supported.
-
-(Note that we applied the [linear learning rate scaling rule](https://arxiv.org/abs/1706.02677)
-when changing the batch size.)
-
-To evaluate this model's performance, use
-```
-python tools/train_net.py \
-	--config-file configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_1x.yaml \
-	--eval-only MODEL.WEIGHTS /path/to/checkpoint_file
-```
-For more options, see `python tools/train_net.py -h`.
-
-### Use Detectron2 in Your Code
-
-See our [Colab Notebook](https://colab.research.google.com/drive/16jcaJoc6bCFAQ96jDe2HwtXj7BMD_-m5)
-to learn how to use detectron2 APIs to:
-1. run inference with an existing model
-2. train a builtin model on a custom dataset
-
-See [detectron2/projects](https://github.com/facebookresearch/detectron2/tree/master/projects)
-for more ways to build your project on detectron2.
diff --git a/docs/tutorials/getting_started.md b/docs/tutorials/getting_started.md
new file mode 120000
index 0000000..e90bde7
--- /dev/null
+++ b/docs/tutorials/getting_started.md
@@ -0,0 +1 @@
+../../GETTING_STARTED.md
\ No newline at end of file
diff --git a/docs/tutorials/install.md b/docs/tutorials/install.md
deleted file mode 100644
index 458a838..0000000
--- a/docs/tutorials/install.md
+++ /dev/null
@@ -1,69 +0,0 @@
-## Installation
-
-Our [Colab Notebook](https://colab.research.google.com/drive/16jcaJoc6bCFAQ96jDe2HwtXj7BMD_-m5)
-has step-by-step instructions that install detectron2.
-The [Dockerfile](https://github.com/facebookresearch/detectron2/blob/master/Dockerfile)
-also installs detectron2 with a few simple commands.
-
-### Requirements
-- Linux or macOS
-- Python >= 3.6
-- PyTorch 1.3
-- [torchvision](https://github.com/pytorch/vision/) that matches the PyTorch installation.
-	You can install them together at [pytorch.org](https://pytorch.org) to make sure of this.
-- OpenCV, needed by demo and visualization
-- [fvcore](https://github.com/facebookresearch/fvcore/): `pip install 'git+https://github.com/facebookresearch/fvcore'`
-- pycocotools: `pip install cython; pip install 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'`
-- GCC >= 4.9
-
-
-### Build Detectron2
-
-After having the above dependencies, run:
-```
-git clone git@github.com:facebookresearch/detectron2.git
-cd detectron2
-python setup.py build develop
-
-# or if you are on macOS
-# MACOSX_DEPLOYMENT_TARGET=10.9 CC=clang CXX=clang++ python setup.py build develop
-
-# or, as an alternative to `setup.py`, do
-# pip install [--editable] .
-```
-Note: you may need to rebuild detectron2 after reinstalling a different build of PyTorch.
-
-### Common Installation Issues
-
-+ Undefined torch/aten symbols, or segmentation fault immediately when running the library.
-  This may be caused by the following reasons:
-
-	* detectron2 or torchvision is not compiled with the version of PyTorch you're running.
-
-		If you use a pre-built torchvision, uninstall torchvision & pytorch, and reinstall them
-		following [pytorch.org](http://pytorch.org).
-		If you manually build detectron2 or torchvision, remove the files you built (`build/`, `**/*.so`)
-		and rebuild them.
-
-	* detectron2 or torchvision is not compiled using gcc >= 4.9.
-
-	  You'll see a warning message during compilation in this case. Please remove the files you build,
-		and rebuild them.
-		Technically, you need the identical compiler that's used to build pytorch to guarantee
-		compatibility. But in practice, gcc >= 4.9 should work OK.
-
-+ Undefined cuda symbols. The version of NVCC you use to build detectron2 or torchvision does
-	not match the version of cuda you are running with.
-	This happens sometimes when using anaconda.
-
-+ "Not compiled with GPU support": make sure
-	```
-	python -c 'import torch; from torch.utils.cpp_extension import CUDA_HOME; print(torch.cuda.is_available(), CUDA_HOME)'
-	```
-	print valid outputs at the time you build detectron2.
-
-+ "invalid device function": two possibilities:
-  * You build detectron2 with one version of CUDA but run it with a different version.
-  * Detectron2 is not built with the correct compute compability for the GPU model.
-    The compute compability defaults to match the GPU found on the machine,
-    and can be controlled by `TORCH_CUDA_ARCH_LIST` environment variable during installation.
diff --git a/docs/tutorials/install.md b/docs/tutorials/install.md
new file mode 120000
index 0000000..5f52b2b
--- /dev/null
+++ b/docs/tutorials/install.md
@@ -0,0 +1 @@
+../../INSTALL.md
\ No newline at end of file
